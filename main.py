#!/usr/bin/env python3
"""
Paper Slack Bot - è«–æ–‡ã‚’åé›†ã—ã¦Slackã«æŠ•ç¨¿ã™ã‚‹ãƒœãƒƒãƒˆ
"""

import os
import sys
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from dataclasses import dataclass

import arxiv
from dotenv import load_dotenv
import requests

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()


@dataclass
class Paper:
    """è«–æ–‡ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    title: str
    authors: List[str]
    summary: str
    published: datetime
    url: str
    pdf_url: str
    arxiv_id: str
    citation_count: int = 0
    ai_summary: Optional[str] = None


class ArxivFetcher:
    """arXivã‹ã‚‰è«–æ–‡ã‚’å–å¾—ã™ã‚‹ã‚¯ãƒ©ã‚¹"""

    def __init__(self, query: str, max_results: int = 20):
        self.query = query
        self.max_results = max_results

    def fetch_papers(self, days_back: int = 1) -> List[Paper]:
        """
        éå»Næ—¥ä»¥å†…ã®è«–æ–‡ã‚’å–å¾—

        Args:
            days_back: ä½•æ—¥å‰ã¾ã§ã®è«–æ–‡ã‚’å–å¾—ã™ã‚‹ã‹

        Returns:
            è«–æ–‡ãƒªã‚¹ãƒˆ
        """
        logger.info(f"arXivã‹ã‚‰è«–æ–‡ã‚’å–å¾—ã—ã¾ã™: query={self.query}, max_results={self.max_results}")

        # æ˜¨æ—¥ã®æ—¥ä»˜ã‚’è¨ˆç®—
        since_date = datetime.now() - timedelta(days=days_back)

        # arXivæ¤œç´¢å®Ÿè¡Œ
        search = arxiv.Search(
            query=self.query,
            max_results=self.max_results,
            sort_by=arxiv.SortCriterion.SubmittedDate,
            sort_order=arxiv.SortOrder.Descending
        )

        papers = []
        try:
            for result in search.results():
                # å…¬é–‹æ—¥ãƒ•ã‚£ãƒ«ã‚¿
                if result.published.replace(tzinfo=None) < since_date:
                    continue

                paper = Paper(
                    title=result.title,
                    authors=[a.name for a in result.authors],
                    summary=result.summary.replace('\n', ' '),
                    published=result.published,
                    url=result.entry_id,
                    pdf_url=result.pdf_url,
                    arxiv_id=result.entry_id.split('/')[-1]
                )
                papers.append(paper)

            logger.info(f"{len(papers)}ä»¶ã®è«–æ–‡ã‚’å–å¾—ã—ã¾ã—ãŸ")
            return papers

        except Exception as e:
            logger.error(f"è«–æ–‡å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []


class SemanticScholarClient:
    """Semantic Scholar APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""

    def __init__(self, api_key: Optional[str] = None):
        self.base_url = "https://api.semanticscholar.org/graph/v1"
        self.api_key = api_key
        self.headers = {}
        if api_key:
            self.headers["x-api-key"] = api_key

    def get_paper_details(self, arxiv_id: str) -> Dict:
        """
        arXiv IDã‹ã‚‰è«–æ–‡è©³ç´°ã‚’å–å¾—

        Args:
            arxiv_id: arXiv IDï¼ˆä¾‹: "2301.00001"ï¼‰

        Returns:
            è«–æ–‡è©³ç´°æƒ…å ±
        """
        url = f"{self.base_url}/paper/arXiv:{arxiv_id}"
        params = {
            "fields": "citationCount,influence,year,title"
        }

        try:
            response = requests.get(url, params=params, headers=self.headers, timeout=10)
            response.raise_for_status()
            return response.json()
        except Exception as e:
            logger.debug(f"Semantic Scholarå–å¾—ã‚¨ãƒ©ãƒ¼ ({arxiv_id}): {e}")
            return {}

    def enrich_papers(self, papers: List[Paper]) -> List[Paper]:
        """
        è«–æ–‡ãƒªã‚¹ãƒˆã«å¼•ç”¨æ•°ãªã©ã®æƒ…å ±ã‚’ä»˜ä¸

        Args:
            papers: è«–æ–‡ãƒªã‚¹ãƒˆ

        Returns:
        æƒ…å ±ä»˜ä¸æ¸ˆã¿ã®è«–æ–‡ãƒªã‚¹ãƒˆ
        """
        logger.info("Semantic Scholarã§è«–æ–‡æƒ…å ±ã‚’ä»˜ä¸ã—ã¾ã™")

        for paper in papers:
            details = self.get_paper_details(paper.arxiv_id)
            if details:
                paper.citation_count = details.get("citationCount", 0)

        return papers


class LLMSummarizer:
    """LLMã§è¦ç´„ã‚’ç”Ÿæˆã™ã‚‹ã‚¯ãƒ©ã‚¹"""

    def __init__(self, api_key: str, model: str = "gpt-4o-mini", max_length: int = 200):
        try:
            from openai import OpenAI
            self.client = OpenAI(api_key=api_key)
            self.model = model
            self.max_length = max_length
            self.enabled = True
        except ImportError:
            logger.warning("OpenAIãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚è¦ç´„ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            self.enabled = False
        except Exception as e:
            logger.warning(f"OpenAIåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}ã€‚è¦ç´„ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            self.enabled = False

    def summarize(self, paper: Paper) -> str:
        """
        è«–æ–‡ã®è¦ç´„ã‚’ç”Ÿæˆï¼ˆæ—¥æœ¬èªï¼‰

        Args:
            paper: è«–æ–‡

        Returns:
            è¦ç´„ãƒ†ã‚­ã‚¹ãƒˆ
        """
        if not self.enabled:
            return None

        try:
            prompt = f"""ä»¥ä¸‹ã®è«–æ–‡ã®è¦ç´„ã‚’æ—¥æœ¬èªã§{self.max_length}æ–‡å­—ä»¥å†…ã§ç°¡æ½”ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚

ã‚¿ã‚¤ãƒˆãƒ«: {paper.title}

è¦ç´„:
{paper.summary}

é‡è¦ãªè²¢çŒ®ã¨ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆã‚’ä¸­å¿ƒã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚"""

            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ã‚ãªãŸã¯è«–æ–‡ã®è¦ç´„ã‚’ä½œæˆã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=500,
                temperature=0.3
            )

            return response.choices[0].message.content.strip()

        except Exception as e:
            logger.error(f"è¦ç´„ç”Ÿæˆã‚¨ãƒ©ãƒ¼ ({paper.arxiv_id}): {e}")
            return None


class SlackNotifier:
    """Slackã«é€šçŸ¥ã‚’é€ã‚‹ã‚¯ãƒ©ã‚¹"""

    def __init__(self, webhook_url: str):
        self.webhook_url = webhook_url

    def send_papers(self, papers: List[Paper], channel_name: str = "è«–æ–‡ãƒœãƒƒãƒˆ") -> bool:
        """
        è«–æ–‡ãƒªã‚¹ãƒˆã‚’Slackã«é€ä¿¡

        Args:
            papers: è«–æ–‡ãƒªã‚¹ãƒˆ
            channel_name: ãƒãƒ£ãƒ³ãƒãƒ«åï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼ç”¨ï¼‰

        Returns:
            æˆåŠŸã‹ã©ã†ã‹
        """
        if not papers:
            logger.info("é€ä¿¡ã™ã‚‹è«–æ–‡ãŒã‚ã‚Šã¾ã›ã‚“")
            return True

        today = datetime.now().strftime("%Y/%m/%d")
        count = len(papers)

        # ãƒ¡ã‚¤ãƒ³ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ§‹ç¯‰
        header = {
            "type": "header",
            "text": {
                "type": "plain_text",
                "text": f"ğŸ”¥ {today} äººæ°—è«–æ–‡ Top{count}"
            }
        }

        blocks = [header]

        # è«–æ–‡ã”ã¨ã®ãƒ–ãƒ­ãƒƒã‚¯
        for i, paper in enumerate(papers, 1):
            citation_info = f" | å¼•ç”¨{paper.citation_count}å›" if paper.citation_count > 0 else ""

            # è¦ç´„ãŒã‚ã‚‹å ´åˆ
            summary_text = paper.ai_summary if paper.ai_summary else self._truncate_text(paper.summary, 200)

            paper_block = {
                "type": "section",
                "text": {
                    "type": "mrkdwn",
                    "text": f"*{i}. {paper.title}*\n"
                            f"_{'ã€'.join(paper.authors[:3])}{'ä»–' if len(paper.authors) > 3 else ''}_\n"
                            f"{summary_text}{citation_info}\n"
                            f"<{paper.url}|arXiv> | <{paper.pdf_url}|PDF>"
                }
            }
            blocks.append(paper_block)
            blocks.append({"type": "divider"})

        # é€ä¿¡
        payload = {"blocks": blocks}

        try:
            response = requests.post(self.webhook_url, json=payload, timeout=10)
            response.raise_for_status()
            logger.info(f"Slackã«é€ä¿¡ã—ã¾ã—ãŸ: {count}ä»¶")
            return True
        except Exception as e:
            logger.error(f"Slacké€ä¿¡ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def _truncate_text(self, text: str, max_length: int) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’æŒ‡å®šé•·ã•ã«åˆ‡ã‚Šè©°ã‚ã‚‹"""
        if len(text) <= max_length:
            return text
        return text[:max_length-3] + "..."


class EmailNotifier:
    """Emailã§é€šçŸ¥ã‚’é€ã‚‹ã‚¯ãƒ©ã‚¹ï¼ˆResendä½¿ç”¨ï¼‰"""

    def __init__(self, api_key: str, from_email: str, to_email: str):
        self.api_key = api_key
        self.from_email = from_email
        self.to_email = to_email
        self.base_url = "https://api.resend.com/emails"

    def send_papers(self, papers: List[Paper]) -> bool:
        """
        è«–æ–‡ãƒªã‚¹ãƒˆã‚’Emailã§é€ä¿¡

        Args:
            papers: è«–æ–‡ãƒªã‚¹ãƒˆ

        Returns:
            æˆåŠŸã‹ã©ã†ã‹
        """
        if not papers:
            logger.info("é€ä¿¡ã™ã‚‹è«–æ–‡ãŒã‚ã‚Šã¾ã›ã‚“")
            return True

        today = datetime.now().strftime("%Y/%m/%d")
        count = len(papers)

        # HTMLãƒ¡ãƒ¼ãƒ«æ§‹ç¯‰
        html_parts = [f"<h2>ğŸ”¥ {today} äººæ°—è«–æ–‡ Top{count}</h2>"]

        for i, paper in enumerate(papers, 1):
            citation_info = f" | å¼•ç”¨{paper.citation_count}å›" if paper.citation_count > 0 else ""
            summary_text = paper.ai_summary if paper.ai_summary else paper.summary[:300] + "..."

            html_parts.append(f"""
            <div style="margin: 20px 0; padding: 15px; border: 1px solid #ddd; border-radius: 8px;">
                <h3>{i}. {paper.title}</h3>
                <p><em>{', '.join(paper.authors[:3])}{' et al.' if len(paper.authors) > 3 else ''}</em></p>
                <p>{summary_text}{citation_info}</p>
                <p>
                    <a href="{paper.url}">arXiv</a> | <a href="{paper.pdf_url}">PDF</a>
                </p>
            </div>
            """)

        html_content = f"""
        <html>
        <body style="font-family: Arial, sans-serif; max-width: 800px; margin: 0 auto;">
            {''.join(html_parts)}
        </body>
        </html>
        """

        # é€ä¿¡
        payload = {
            "from": self.from_email,
            "to": [self.to_email],
            "subject": f"ğŸ”¥ {today} äººæ°—è«–æ–‡ Top{count}",
            "html": html_content
        }

        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }

        try:
            response = requests.post(self.base_url, json=payload, headers=headers, timeout=10)
            response.raise_for_status()
            logger.info(f"Emailã‚’é€ä¿¡ã—ã¾ã—ãŸ: {count}ä»¶")
            return True
        except Exception as e:
            logger.error(f"Emailé€ä¿¡ã‚¨ãƒ©ãƒ¼: {e}")
            return False


def filter_papers(papers: List[Paper], min_citations: int = 0) -> List[Paper]:
    """è«–æ–‡ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
    filtered = [p for p in papers if p.citation_count >= min_citations]
    if len(filtered) < len(papers):
        logger.info(f"å¼•ç”¨æ•°ãƒ•ã‚£ãƒ«ã‚¿: {len(papers)}ä»¶ -> {len(filtered)}ä»¶")
    return filtered


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    logger.info("=" * 50)
    logger.info("Paper Slack Bot é–‹å§‹")
    logger.info("=" * 50)

    # è¨­å®šå–å¾—
    query = os.getenv("ARXIV_QUERY", "cat:cs.AI OR cat:cs.LG")
    max_papers = int(os.getenv("MAX_PAPERS", "100"))
    min_citations = int(os.getenv("MIN_CITATIONS", "0")

    # é€šçŸ¥å…ˆï¼ˆSlack or Emailï¼‰
    webhook_url = os.getenv("SLACK_WEBHOOK_URL")
    resend_api_key = os.getenv("RESEND_API_KEY")
    email_from = os.getenv("EMAIL_FROM", "Paper Daily <papers@yourdomain.com>")
    email_to = os.getenv("EMAIL_TO")

    if not webhook_url and not (resend_api_key and email_to):
        logger.error("é€šçŸ¥å…ˆãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼ˆSLACK_WEBHOOK_URL ã¾ãŸã¯ RESEND_API_KEY + EMAIL_TOï¼‰")
        sys.exit(1)

    # 1. arXivã‹ã‚‰è«–æ–‡å–å¾—
    fetcher = ArxivFetcher(query, max_papers)
    papers = fetcher.fetch_papers(days_back=1)

    if not papers:
        logger.info("æ–°ã—ã„è«–æ–‡ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return

    # 2. Semantic Scholarã§æƒ…å ±ä»˜ä¸
    api_key = os.getenv("SEMANTIC_SCHOLAR_API_KEY")
    if api_key or True:  # APIã‚­ãƒ¼ãªã—ã§ã‚‚ç„¡æ–™æ ã§å‹•ä½œ
        semantic_client = SemanticScholarClient(api_key)
        papers = semantic_client.enrich_papers(papers)

    # 3. ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    papers = filter_papers(papers, min_citations)

    if not papers:
        logger.info("ãƒ•ã‚£ãƒ«ã‚¿å¾Œã€è«–æ–‡ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return

    # äººæ°—é †ï¼ˆå¼•ç”¨æ•°é™é †ï¼‰ã«ã‚½ãƒ¼ãƒˆã—ã¦Top10
    papers = sorted(papers, key=lambda p: p.citation_count, reverse=True)
    papers = papers[:10]
    logger.info(f"äººæ°—Top10: {len(papers)}ä»¶ï¼ˆæœ€é«˜å¼•ç”¨æ•°={papers[0].citation_count if papers else 0}ï¼‰")

    # 4. LLMã§è¦ç´„ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    openai_key = os.getenv("OPENAI_API_KEY")
    if openai_key:
        model = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
        max_length = int(os.getenv("SUMMARY_MAX_LENGTH", "200"))
        summarizer = LLMSummarizer(openai_key, model, max_length)

        if summarizer.enabled:
            logger.info("è¦ç´„ã‚’ç”Ÿæˆã—ã¾ã™...")
            for paper in papers:
                paper.ai_summary = summarizer.summarize(paper)

    # 5. é€šçŸ¥é€ä¿¡
    success_count = 0

    # Slack
    if webhook_url:
        notifier = SlackNotifier(webhook_url)
        if notifier.send_papers(papers):
            success_count += 1

    # Email
    if resend_api_key and email_to:
        notifier = EmailNotifier(resend_api_key, email_from, email_to)
        if notifier.send_papers(papers):
            success_count += 1

    if success_count > 0:
        logger.info(f"å®Œäº†ã—ã¾ã—ãŸï¼ˆ{success_count}ä»¶é€ä¿¡ï¼‰")
    else:
        logger.error("ã™ã¹ã¦ã®é€ä¿¡ã«å¤±æ•—ã—ã¾ã—ãŸ")
        sys.exit(1)


if __name__ == "__main__":
    main()
