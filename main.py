#!/usr/bin/env python3
"""
Paper Slack Bot - è«–æ–‡ã‚’åé›†ã—ã¦Slackã«æŠ•ç¨¿ã™ã‚‹ãƒœãƒƒãƒˆ
"""

import os
import sys
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from dataclasses import dataclass

import arxiv
from dotenv import load_dotenv
import requests

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()


@dataclass
class Paper:
    """è«–æ–‡ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    title: str
    authors: List[str]
    summary: str
    published: datetime
    url: str
    pdf_url: str
    arxiv_id: str
    citation_count: int = 0
    ai_summary: Optional[str] = None


class ArxivFetcher:
    """arXivã‹ã‚‰è«–æ–‡ã‚’å–å¾—ã™ã‚‹ã‚¯ãƒ©ã‚¹"""

    def __init__(self, query: str, max_results: int = 20):
        self.query = query
        self.max_results = max_results

    def fetch_papers(self, days_back: int = 1) -> List[Paper]:
        """
        éå»Næ—¥ä»¥å†…ã®è«–æ–‡ã‚’å–å¾—

        Args:
            days_back: ä½•æ—¥å‰ã¾ã§ã®è«–æ–‡ã‚’å–å¾—ã™ã‚‹ã‹

        Returns:
            è«–æ–‡ãƒªã‚¹ãƒˆ
        """
        logger.info(f"arXivã‹ã‚‰è«–æ–‡ã‚’å–å¾—ã—ã¾ã™: query={self.query}, max_results={self.max_results}")

        # æ˜¨æ—¥ã®æ—¥ä»˜ã‚’è¨ˆç®—
        since_date = datetime.now() - timedelta(days=days_back)

        # arXivæ¤œç´¢å®Ÿè¡Œ
        search = arxiv.Search(
            query=self.query,
            max_results=self.max_results,
            sort_by=arxiv.SortCriterion.SubmittedDate,
            sort_order=arxiv.SortOrder.Descending
        )

        papers = []
        try:
            for result in search.results():
                # å…¬é–‹æ—¥ãƒ•ã‚£ãƒ«ã‚¿
                if result.published.replace(tzinfo=None) < since_date:
                    continue

                paper = Paper(
                    title=result.title,
                    authors=[a.name for a in result.authors],
                    summary=result.summary.replace('\n', ' '),
                    published=result.published,
                    url=result.entry_id,
                    pdf_url=result.pdf_url,
                    arxiv_id=result.entry_id.split('/')[-1]
                )
                papers.append(paper)

            logger.info(f"{len(papers)}ä»¶ã®è«–æ–‡ã‚’å–å¾—ã—ã¾ã—ãŸ")
            return papers

        except Exception as e:
            logger.error(f"è«–æ–‡å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []


class SemanticScholarClient:
    """Semantic Scholar APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""

    def __init__(self, api_key: Optional[str] = None):
        self.base_url = "https://api.semanticscholar.org/graph/v1"
        self.api_key = api_key
        self.headers = {}
        if api_key:
            self.headers["x-api-key"] = api_key

    def get_paper_details(self, arxiv_id: str) -> Dict:
        """
        arXiv IDã‹ã‚‰è«–æ–‡è©³ç´°ã‚’å–å¾—

        Args:
            arxiv_id: arXiv IDï¼ˆä¾‹: "2301.00001"ï¼‰

        Returns:
            è«–æ–‡è©³ç´°æƒ…å ±
        """
        url = f"{self.base_url}/paper/arXiv:{arxiv_id}"
        params = {
            "fields": "citationCount,influence,year,title"
        }

        try:
            response = requests.get(url, params=params, headers=self.headers, timeout=10)
            response.raise_for_status()
            return response.json()
        except Exception as e:
            logger.debug(f"Semantic Scholarå–å¾—ã‚¨ãƒ©ãƒ¼ ({arxiv_id}): {e}")
            return {}

    def enrich_papers(self, papers: List[Paper]) -> List[Paper]:
        """
        è«–æ–‡ãƒªã‚¹ãƒˆã«å¼•ç”¨æ•°ãªã©ã®æƒ…å ±ã‚’ä»˜ä¸

        Args:
            papers: è«–æ–‡ãƒªã‚¹ãƒˆ

        Returns:
        æƒ…å ±ä»˜ä¸æ¸ˆã¿ã®è«–æ–‡ãƒªã‚¹ãƒˆ
        """
        logger.info("Semantic Scholarã§è«–æ–‡æƒ…å ±ã‚’ä»˜ä¸ã—ã¾ã™")

        for paper in papers:
            details = self.get_paper_details(paper.arxiv_id)
            if details:
                paper.citation_count = details.get("citationCount", 0)

        return papers


class HuggingFaceDailyFetcher:
    """Hugging Face Daily Papers APIã§äººæ°—é †ã«è«–æ–‡ã‚’å–å¾—"""

    def __init__(self, limit: int = 50):
        self.base_url = "https://huggingface.co/api/daily_papers"
        self.limit = limit

    def fetch_papers(self, keyword: Optional[str] = None) -> List[Paper]:
        """
        Hugging Face Daily Papersã‹ã‚‰upvotesé †ã«è«–æ–‡å–å¾—

        Args:
            keyword: ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆä¾‹: "RAG"ï¼‰

        Returns:
            è«–æ–‡ãƒªã‚¹ãƒˆï¼ˆupvotesé™é †ï¼‰
        """
        logger.info(f"Hugging Face Daily Papersã‹ã‚‰è«–æ–‡ã‚’å–å¾—ã—ã¾ã™: limit={self.limit}")

        try:
            params = {"limit": self.limit}
            response = requests.get(self.base_url, params=params, timeout=30)
            response.raise_for_status()
            data = response.json()

            papers = []
            for item in data:
                paper_data = item.get("paper", item)
                paper_id = paper_data.get("id", "")

                # Hugging Face IDã‚’arXiv IDã«å¤‰æ›ï¼ˆä¾‹: 2602.02016 -> 2602.02016ï¼‰
                if "." not in paper_id:
                    continue

                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚£ãƒ«ã‚¿
                if keyword:
                    title = paper_data.get("title", "").lower()
                    summary = paper_data.get("summary", "").lower()
                    if keyword.lower() not in title and keyword.lower() not in summary:
                        continue

                paper = Paper(
                    title=paper_data.get("title", ""),
                    authors=[a.get("name", "") for a in paper_data.get("authors", [])],
                    summary=paper_data.get("summary", ""),
                    published=datetime.fromisoformat(paper_data.get("publishedAt", "").replace("Z", "+00:00")),
                    url=f"https://huggingface.co/papers/{paper_id}",
                    pdf_url=f"https://arxiv.org/pdf/{paper_id}.pdf",
                    arxiv_id=paper_id,
                    citation_count=paper_data.get("upvotes", 0),  # upvotesã‚’ã‚¹ã‚³ã‚¢ã¨ã—ã¦ä½¿ç”¨
                    ai_summary=paper_data.get("ai_summary")
                )
                papers.append(paper)

            logger.info(f"Hugging Faceã‹ã‚‰{len(papers)}ä»¶ã®è«–æ–‡ã‚’å–å¾—ã—ã¾ã—ãŸ")
            return papers

        except Exception as e:
            logger.error(f"Hugging Faceå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []


class LLMSummarizer:
    """LLMã§è¦ç´„ã‚’ç”Ÿæˆã™ã‚‹ã‚¯ãƒ©ã‚¹"""

    def __init__(self, api_key: str, model: str = "gpt-4o-mini", max_length: int = 200):
        try:
            from openai import OpenAI
            self.client = OpenAI(api_key=api_key)
            self.model = model
            self.max_length = max_length
            self.enabled = True
        except ImportError:
            logger.warning("OpenAIãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚è¦ç´„ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            self.enabled = False
        except Exception as e:
            logger.warning(f"OpenAIåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}ã€‚è¦ç´„ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            self.enabled = False

    def summarize(self, paper: Paper) -> str:
        """
        è«–æ–‡ã®è¦ç´„ã‚’ç”Ÿæˆï¼ˆæ—¥æœ¬èªï¼‰

        Args:
            paper: è«–æ–‡

        Returns:
            è¦ç´„ãƒ†ã‚­ã‚¹ãƒˆ
        """
        if not self.enabled:
            return None

        try:
            prompt = f"""ä»¥ä¸‹ã®è«–æ–‡ã®è¦ç´„ã‚’æ—¥æœ¬èªã§{self.max_length}æ–‡å­—ä»¥å†…ã§ç°¡æ½”ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚

ã‚¿ã‚¤ãƒˆãƒ«: {paper.title}

è¦ç´„:
{paper.summary}

é‡è¦ãªè²¢çŒ®ã¨ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆã‚’ä¸­å¿ƒã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚"""

            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ã‚ãªãŸã¯è«–æ–‡ã®è¦ç´„ã‚’ä½œæˆã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=500,
                temperature=0.3
            )

            return response.choices[0].message.content.strip()

        except Exception as e:
            logger.error(f"è¦ç´„ç”Ÿæˆã‚¨ãƒ©ãƒ¼ ({paper.arxiv_id}): {e}")
            return None


class SlackNotifier:
    """Slackã«é€šçŸ¥ã‚’é€ã‚‹ã‚¯ãƒ©ã‚¹"""

    def __init__(self, webhook_url: str):
        self.webhook_url = webhook_url

    def send_papers(self, papers: List[Paper], channel_name: str = "è«–æ–‡ãƒœãƒƒãƒˆ") -> bool:
        """
        è«–æ–‡ãƒªã‚¹ãƒˆã‚’Slackã«é€ä¿¡

        Args:
            papers: è«–æ–‡ãƒªã‚¹ãƒˆ
            channel_name: ãƒãƒ£ãƒ³ãƒãƒ«åï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼ç”¨ï¼‰

        Returns:
            æˆåŠŸã‹ã©ã†ã‹
        """
        if not papers:
            logger.info("é€ä¿¡ã™ã‚‹è«–æ–‡ãŒã‚ã‚Šã¾ã›ã‚“")
            return True

        today = datetime.now().strftime("%Y/%m/%d")
        count = len(papers)

        # ãƒ¡ã‚¤ãƒ³ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ§‹ç¯‰
        header = {
            "type": "header",
            "text": {
                "type": "plain_text",
                "text": f"ğŸ”¥ {today} äººæ°—è«–æ–‡ Top{count}"
            }
        }

        blocks = [header]

        # è«–æ–‡ã”ã¨ã®ãƒ–ãƒ­ãƒƒã‚¯
        for i, paper in enumerate(papers, 1):
            citation_info = f" | å¼•ç”¨{paper.citation_count}å›" if paper.citation_count > 0 else ""

            # è¦ç´„ãŒã‚ã‚‹å ´åˆ
            summary_text = paper.ai_summary if paper.ai_summary else self._truncate_text(paper.summary, 200)

            paper_block = {
                "type": "section",
                "text": {
                    "type": "mrkdwn",
                    "text": f"*{i}. {paper.title}*\n"
                            f"_{'ã€'.join(paper.authors[:3])}{'ä»–' if len(paper.authors) > 3 else ''}_\n"
                            f"{summary_text}{citation_info}\n"
                            f"<{paper.url}|arXiv> | <{paper.pdf_url}|PDF>"
                }
            }
            blocks.append(paper_block)
            blocks.append({"type": "divider"})

        # é€ä¿¡
        payload = {"blocks": blocks}

        try:
            response = requests.post(self.webhook_url, json=payload, timeout=10)
            response.raise_for_status()
            logger.info(f"Slackã«é€ä¿¡ã—ã¾ã—ãŸ: {count}ä»¶")
            return True
        except Exception as e:
            logger.error(f"Slacké€ä¿¡ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def _truncate_text(self, text: str, max_length: int) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’æŒ‡å®šé•·ã•ã«åˆ‡ã‚Šè©°ã‚ã‚‹"""
        if len(text) <= max_length:
            return text
        return text[:max_length-3] + "..."


class EmailNotifier:
    """Emailã§é€šçŸ¥ã‚’é€ã‚‹ã‚¯ãƒ©ã‚¹ï¼ˆResendä½¿ç”¨ï¼‰"""

    def __init__(self, api_key: str, from_email: str, to_email: str):
        self.api_key = api_key
        self.from_email = from_email
        self.to_email = to_email
        self.base_url = "https://api.resend.com/emails"

    def send_papers_sections(self, papers_sections: List[tuple]) -> bool:
        """
        è¤‡æ•°ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®è«–æ–‡ãƒªã‚¹ãƒˆã‚’Emailã§é€ä¿¡

        Args:
            papers_sections: [(ã‚»ã‚¯ã‚·ãƒ§ãƒ³å, è«–æ–‡ãƒªã‚¹ãƒˆ), ...] ã®ãƒªã‚¹ãƒˆ

        Returns:
            æˆåŠŸã‹ã©ã†ã‹
        """
        if not papers_sections:
            logger.info("é€ä¿¡ã™ã‚‹è«–æ–‡ãŒã‚ã‚Šã¾ã›ã‚“")
            return True

        today = datetime.now().strftime("%Y/%m/%d")
        total_count = sum(len(papers) for _, papers in papers_sections)

        # HTMLãƒ¡ãƒ¼ãƒ«æ§‹ç¯‰
        html_parts = [f"<h1>ğŸ”¥ {today} AIè«–æ–‡ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆå…¨{total_count}ä»¶ï¼‰</h1>"]

        for section_name, papers in papers_sections:
            if not papers:
                continue

            html_parts.append(f"<h2>ğŸ“š {section_name}ï¼ˆ{len(papers)}ä»¶ï¼‰</h2>")

            for i, paper in enumerate(papers, 1):
                # Hugging Face URLåˆ¤å®š
                hf_url = f"https://huggingface.co/papers/{paper.arxiv_id}" if "huggingface.co" in paper.url else paper.url

                upvote_info = f" | ğŸ‘ {paper.citation_count} upvotes" if paper.citation_count > 0 else ""
                summary_text = paper.ai_summary if paper.ai_summary else paper.summary[:300] + "..."

                html_parts.append(f"""
                <div style="margin: 20px 0; padding: 15px; border: 1px solid #ddd; border-radius: 8px;">
                    <h3>{i}. {paper.title}</h3>
                    <p><em>{', '.join(paper.authors[:3])}{' et al.' if len(paper.authors) > 3 else ''}</em></p>
                    <p>{summary_text}{upvote_info}</p>
                    <p>
                        <a href="{hf_url}">Hugging Face</a> | <a href="https://arxiv.org/abs/{paper.arxiv_id}">arXiv</a> | <a href="{paper.pdf_url}">PDF</a>
                    </p>
                </div>
                """)

        html_content = f"""
        <html>
        <body style="font-family: Arial, sans-serif; max-width: 800px; margin: 0 auto; padding: 20px;">
            {''.join(html_parts)}
            <hr style="margin-top: 30px;">
            <p style="color: #666; font-size: 12px;">
                Powered by <a href="https://huggingface.co/papers">Hugging Face Papers</a>
            </p>
        </body>
        </html>
        """

        # é€ä¿¡
        payload = {
            "from": self.from_email,
            "to": [self.to_email],
            "subject": f"ğŸ”¥ {today} AIè«–æ–‡ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆå…¨{total_count}ä»¶ï¼‰",
            "html": html_content
        }

        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }

        try:
            response = requests.post(self.base_url, json=payload, headers=headers, timeout=10)
            response.raise_for_status()
            logger.info(f"Emailã‚’é€ä¿¡ã—ã¾ã—ãŸ: {total_count}ä»¶")
            return True
        except Exception as e:
            logger.error(f"Emailé€ä¿¡ã‚¨ãƒ©ãƒ¼: {e}")
            return False


def filter_papers(papers: List[Paper], min_citations: int = 0) -> List[Paper]:
    """è«–æ–‡ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
    filtered = [p for p in papers if p.citation_count >= min_citations]
    if len(filtered) < len(papers):
        logger.info(f"å¼•ç”¨æ•°ãƒ•ã‚£ãƒ«ã‚¿: {len(papers)}ä»¶ -> {len(filtered)}ä»¶")
    return filtered


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    logger.info("=" * 50)
    logger.info("Paper Slack Bot é–‹å§‹")
    logger.info("=" * 50)

    # è¨­å®šå–å¾—
    query = os.getenv("ARXIV_QUERY", "cat:cs.AI OR cat:cs.LG")
    max_papers = int(os.getenv("MAX_PAPERS", "100"))
    min_citations = int(os.getenv("MIN_CITATIONS", "0"))
    use_huggingface = os.getenv("USE_HUGGINGFACE", "true").lower() == "true"
    keyword_filter = os.getenv("KEYWORD_FILTER", "")  # ä¾‹: "RAG" ã§RAGé–¢é€£ã®ã¿

    # é€šçŸ¥å…ˆï¼ˆSlack or Emailï¼‰
    webhook_url = os.getenv("SLACK_WEBHOOK_URL")
    resend_api_key = os.getenv("RESEND_API_KEY")
    email_from = os.getenv("EMAIL_FROM", "Paper Daily <papers@yourdomain.com>")
    email_to = os.getenv("EMAIL_TO")

    if not webhook_url and not (resend_api_key and email_to):
        logger.error("é€šçŸ¥å…ˆãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼ˆSLACK_WEBHOOK_URL ã¾ãŸã¯ RESEND_API_KEY + EMAIL_TOï¼‰")
        sys.exit(1)

    # 1. è«–æ–‡å–å¾—ï¼ˆHugging Face or arXivï¼‰
    all_papers_sections = []  # è¤‡æ•°ã‚»ã‚¯ã‚·ãƒ§ãƒ³ç”¨

    if use_huggingface:
        logger.info("Hugging Face Daily Papersã‚’ä½¿ç”¨ã—ã¾ã™")
        fetcher = HuggingFaceDailyFetcher(limit=max_papers)

        # é€šå¸¸ã®Top10
        general_papers = fetcher.fetch_papers(keyword=None)
        if general_papers:
            all_papers_sections.append(("äººæ°—Top10", general_papers[:10]))

        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é–¢é€£ã®Top10
        if keyword_filter:
            keyword_papers = fetcher.fetch_papers(keyword=keyword_filter)
            if keyword_papers:
                all_papers_sections.append((f"{keyword_filter} Top10", keyword_papers[:10]))

        if not all_papers_sections:
            logger.info("æ–°ã—ã„è«–æ–‡ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return

    else:
        logger.info("arXiv APIã‚’ä½¿ç”¨ã—ã¾ã™")
        fetcher = ArxivFetcher(query, max_papers)
        papers = fetcher.fetch_papers(days_back=1)

        if not papers:
            logger.info("æ–°ã—ã„è«–æ–‡ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return

        all_papers_sections.append(("äººæ°—Top10", papers))

    # 2. Semantic Scholarã§æƒ…å ±ä»˜ä¸ & 3. ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆarXivã®å ´åˆï¼‰
    if not use_huggingface:
        api_key = os.getenv("SEMANTIC_SCHOLAR_API_KEY")
        if api_key or True:
            semantic_client = SemanticScholarClient(api_key)
            for i, (section_name, papers) in enumerate(all_papers_sections):
                all_papers_sections[i] = (section_name, semantic_client.enrich_papers(papers))

        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼†ã‚½ãƒ¼ãƒˆ
        processed_sections = []
        for section_name, papers in all_papers_sections:
            papers = filter_papers(papers, min_citations)
            if papers:
                papers = sorted(papers, key=lambda p: p.citation_count, reverse=True)
                papers = papers[:10]
                processed_sections.append((section_name, papers))
        all_papers_sections = processed_sections

    # 4. LLMã§è¦ç´„ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    openai_key = os.getenv("OPENAI_API_KEY")
    if openai_key:
        model = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
        max_length = int(os.getenv("SUMMARY_MAX_LENGTH", "200"))
        summarizer = LLMSummarizer(openai_key, model, max_length)

        if summarizer.enabled:
            logger.info("è¦ç´„ã‚’ç”Ÿæˆã—ã¾ã™...")
            for section_name, papers in all_papers_sections:
                for paper in papers:
                    if not paper.ai_summary:
                        paper.ai_summary = summarizer.summarize(paper)

    # 5. é€šçŸ¥é€ä¿¡
    success_count = 0
    total_papers = sum(len(papers) for _, papers in all_papers_sections)

    # Email
    if resend_api_key and email_to:
        notifier = EmailNotifier(resend_api_key, email_from, email_to)
        if notifier.send_papers_sections(all_papers_sections):
            success_count += 1

    # Slackï¼ˆæœ€åˆã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ã¿é€ä¿¡ï¼‰
    if webhook_url and all_papers_sections:
        notifier = SlackNotifier(webhook_url)
        if notifier.send_papers(all_papers_sections[0][1]):
            success_count += 1

    if success_count > 0:
        logger.info(f"å®Œäº†ã—ã¾ã—ãŸï¼ˆ{success_count}ä»¶é€ä¿¡ã€å…¨{total_papers}ä»¶ï¼‰")
    else:
        logger.error("ã™ã¹ã¦ã®é€ä¿¡ã«å¤±æ•—ã—ã¾ã—ãŸ")
        sys.exit(1)


if __name__ == "__main__":
    main()
